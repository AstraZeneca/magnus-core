# Overview

Catalog provides a way to store and retrieve data generated by the individual steps of the dag either to downstream
steps of the dag. Catalog also provides a way to reproduce a historic magnus run on any other machine. Along with
the actual file, we also store the SHA id of the data catalog object in the logs to enable diagnostics.

Magnus stores the data generated for every run in the catalog indexed by the unique run_id of the run. This enables
you to re-run an older run and debug in case of any errors with the actual datasets used.

---
!!! Note

    Since the data is stored per-run, it might cause the catalog to inflate a lot. Please consider some clean up
    mechanisms to regularly prune runs that are no longer relevant.
---

As with all services of magnus, there are several providers of catalog and you can easily extend to create your own
cataloging system and use it in your runs.

## Configuration

Configuring the catalog can be done as follows.

```yaml
catalog:
  type:
  config:
```

### type

The type of catalog you want. This should be one of the catalog types already available.

By default FileSystem Catalog is given if no config is provided.

### config

Any configuration variables accepted by the catalog provider.

## Configuration within Step

Within a step of the dag, the catalog can be configured by

```yaml
catalog:
  ...

dag:
  steps:
    step name:
      ...
      catalog:
        compute_data_folder: # optional
        get:
          - list
        put:
          - list

    ...
```

or via the Python SDK:

```python
from magnus import Task

catalog = {
  'compute_data_folder': '' #Â point to the directory from where the data should be extracted
  'get': [], # All the files to get from the catalog
  'put': [] # All the files to put in the catalog
}

first = Task(name='Cool function', command='my_module.my_cool_function', catalog=catalog)

```

### compute_data_folder

The ```compute_data_folder``` for a single step could be different from the global ```compute_data_folder```
and you can provide it by using the catalog settings for that step.

The actual cataloging is done in two stages:

- get: Get the data mentioned in the ```get``` from the catalog to ```compute_data_folder``` before executing the node.
- put: Store all the data mentioned in ```put``` from the ```compute_data_folder``` to catalog after executing the node.

Both ```get``` and ```put``` can accept glob patterns. Internally we use
[Pathlib match function](https://docs.python.org/3/library/pathlib.html#pathlib.PurePath.match)
to match the name to pattern.

---
!!! Note

    The ```put``` stage of the cataloging checks if the data source has been obtained from ```get``` phase and
    only puts a new record if there were changes observed during the execution of the node.
---

## Interaction within code

You can also interact with the catalog within your python programs if it is convenient than providing it in yaml.

### Get from catalog

To get a file from the catalog, use ```get_from_catalog``` from magnus.

For example, the below code gets the file ```interesting_data.csv``` from the catalog into ```data/``` folder.


```python
from magnus import get_from_catalog

def my_function():
  get_from_catalog('interesting.csv', destination_folder='data/')

```

### Put in catalog

To put a file into the catalog, use ```put_in_catalog``` from magnus.

For example, the below code puts the file ```data/interesting_data.csv``` from the data folder into catalog.


```python
from magnus import put_in_catalog

def my_function():
  put_in_catalog('data/interesting.csv')

```

---
!!! Note

    Unlike ```put``` phase of the cataloging process, put_in_catalog does not check if the cataloging object has
    changed and does a blind update.

---

## Passing Data Objects

While the is good for files, it is inconvenient to dump and load the object into files for the cataloging to happen.
Magnus provides utility functions to make it easier.

### Get object from catalog

To get a object from the catalog, use ```get_object``` from magnus.

For example, the below code gets a pandas dataframe from previous steps, called ```interesting_data``` from the catalog.


```python
from magnus import get_object

def my_function():
  df = get_object("interesting_data")

```

Be aware that, the function would raise an exception if ```interesting_data``` was not added to catalog before.

### Put object in catalog

To put a object into the catalog, use ```put_object``` from magnus.

For example, the below code puts the dataframe ```interesting_data``` into the catalog as ```interesting_data.pickle```.


```python
from magnus import put_object

def my_function():
  put_object(data=interesting_data, name="interesting_data")

```

---
!!! Note

    We internally use pickle for the serialization and deserialization. Please raise a feature request if you need
    other kind of serializers.

---


## Parameterized definition

As with any part of the magnus configuration, you can parameterize the configuration of catalog to switch between
catalog providers without changing the base definition.

Please follow the example provided [here](../dag/#parameterized_definition) for more information.


## Extensions

You can easily extend magnus to bring in your custom provider, if a default
implementation does not exist or you are not happy with the implementation.

The ```BaseCatalog``` implementation is as follows:

```python
# You can find the source code in magnus/catalog.py

from pydantic import BaseModel

class BaseCatalog:
    """
    Base Catalog class definition.

    All implementations of the catalog handler should inherit and extend this class.
    """
    service_name = ''

    class Config(BaseModel):
        compute_data_folder: str = defaults.COMPUTE_DATA_FOLDER

    def __init__(self, config: dict, **kwargs):
        config = config or {}
        self.config = self.Config(**config)

    @property
    def compute_data_folder(self) -> str:
        """
        Returns the compute data folder defined as per the config of the catalog.

        Returns:
            [str]: The compute data folder as defined or defaults to magnus default 'data/'
        """
        return self.config.compute_data_folder

    def get(self, name: str, run_id: str, compute_data_folder=None, **kwargs) -> List[object]:
        """
        Get the catalog item by 'name' for the 'run id' and store it in compute data folder.

        The catalog location should have been created before you can get from it.

        Args:
            name (str): The name of the catalog item
            run_id (str): The run_id of the run.
            compute_data_folder (str, optional): The compute data folder. Defaults to magnus default (data/)

        Raises:
            NotImplementedError: Base class, hence not implemented

        Returns:
            List(object) : A list of catalog objects
        """
        raise NotImplementedError

    def put(self, name: str, run_id: str, compute_data_folder=None, synced_catalogs=None, **kwargs) -> List[object]:

        """
        Put the file by 'name' from the 'compute_data_folder' in the catalog for the run_id.

        If previous syncing has happened and the file has not been changed, we do not sync again.

        Args:
            name (str): The name of the catalog item.
            run_id (str): The run_id of the run.
            compute_data_folder (str, optional): The compute data folder. Defaults to magnus default (data/)
            synced_catalogs (dict, optional): Any previously synced catalogs. Defaults to None.

        Raises:
            NotImplementedError: Base class, hence not implemented

        Returns:
            List(object) : A list of catalog objects
        """
        raise NotImplementedError

    def sync_between_runs(self, previous_run_id: str, run_id: str):
        """
        Given run_id of a previous run, sync them to the catalog of the run given by run_id.

        This function is used for re-running older runs.

        Args:
            previous_run_id (str): The run id of the previous run
            run_id (str): The run_id to which the data catalogs should be synced to.

        Raises:
            NotImplementedError: Base class, hence not implemented
        """
        raise NotImplementedError
```


The custom extensions should be registered as part of the namespace: ```catalog``` for it to be
loaded.

```toml
# For example, as part of your pyproject.toml
[tool.poetry.plugins."catalog"]
"gfs" = "YOUR_PACKAGE:GFStorage"
```
