# Example Deployment

While the previous two sections were about introducing magnus pipelines and different features, we can use the same
trivial example to showcase the features of magnus in deployment patterns.

To recap, here is the pipeline that we ran as an example:

--8<--
README.md:exampleInput
--8<--


The pipeline is simple and demonstrates the core concepts of data catalog, dag traversal, passing data between
nodes and task types.

To demonstrate the strength of magnus, let us try to "deploy" the pipeline via a Bash shell script. This demonstration,
though trivial, is very similar in process to *transpile* a dag into something that argo or AWS step functions
understands.

Let us create a configuration file which changes the behavior of magnus to transpile.

```yaml
# config.yaml
executor:
  type: demo-renderer

run_log_store:
  type: file-system

catalog:
  type: file-system

```

**Points to note**:

- We have not changed the dag definition at all.

- We added a config file which instructs the execution type to "demo-renderer".
[Demo renderer](../../concepts/modes-implementations/demo-renderer) translates the dag definition into a bash script.

- The buffered run log store that we have so far used in the example is not suitable anymore.
[File system](../../concepts/run-log-implementations/file-system/) run log store persists the logs
on physical folder and therefore more suitable.


## Transpilation


We can execute the pipeline, just like we did it previously, by the following command.

```magnus execute --file getting-started.yaml --parameters-file parameters.yaml -config-file config.yaml```

or in python SDK:

```python
#in pipeline.py
from magnus import Pipeline, Task

def pipeline():
    first = Task(name='step parameters', command="lambda x: {'x': int(x) + 1}", command_type='python-lambda',
                next_node='step shell')
    second = Task(name='step shell', command='mkdir data ; env >> data/data.txt',
                  command_type='shell', catalog={'put': '*'})

    pipeline = Pipeline(start_at=first, name='getting_started')
    pipeline.construct([first, second])
    pipeline.execute(parameters_file='parameters.yaml', configuration_file='config.yaml')

if __name__ == '__main__':
    pipeline()
```

This run is different from the previous execution

1. There is no output or run_id generated by magus. This is because the current execution only performs a translation
of the dag into a bash script and not actual function calls.

2. You should also notice a file called ```demo-bash.sh``` created in the working directory which is a
translation of the dag into a bash script.


Let us have a closer look at the contents of the ```demo-bash.sh```.

```shell
magnus execute_single_node $1 step%parameters --log-level WARNING --file pipeline.yaml --config-file config.yaml --parameters-file parameters.yaml
exit_code=$?
echo $exit_code
if [ $exit_code -ne 0 ];
then
	 $(magnus execute_single_node $1 fail --log-level WARNING --file pipeline.yaml --config-file config.yaml --parameters-file parameters.yaml)
	exit 1
fi
magnus execute_single_node $1 step%shell --log-level WARNING --file pipeline.yaml --config-file config.yaml --parameters-file parameters.yaml
exit_code=$?
echo $exit_code
if [ $exit_code -ne 0 ];
then
	 $(magnus execute_single_node $1 fail --log-level WARNING --file pipeline.yaml --config-file config.yaml --parameters-file parameters.yaml)
	exit 1
fi
magnus execute_single_node $1 success --log-level WARNING --file pipeline.yaml --config-file config.yaml --parameters-file parameters.yaml```
```

The shell script does the following

- Capture the command line arguments passed to the bash script as magnus parameters, i.e prefixed by MAGNUS_PRM_.

- Execute the first node and capture the exit code. If the exit code is successful, move to ```next``` node as defined
in the dag.

- If the exit code is failure, move to the failure node of the dag. This is as per the dag definition.

## Execution

We can execute the pipeline defined in the ```demo-bash.sh``` by

```shell
chmod 755 demo-bash.sh

./demo-bash.sh my_first_bash
```

**Points to note**

1. run_id, my_first_bash, is no longer optional parameter and should be provided as the first positional parameter.

2). The parameters file was part of the translation step and is provided to the shell script.

Since the run log store is ```file-system```, there should be a directory, ```.run_log_store```, created with a single
run log in it by the name ```my_first_bash.json```.

<details>
  <summary>Click to show the run log</summary>

```json
{
    "run_id": "demo-bash6",
    "dag_hash": "ce0676d63e99c34848484f2df1744bab8d45e33a",
    "use_cached": false,
    "tag": "",
    "original_run_id": "",
    "status": "SUCCESS",
    "steps": {
        "step parameters": {
            "name": "step parameters",
            "internal_name": "step parameters",
            "status": "SUCCESS",
            "step_type": "task",
            "message": "",
            "mock": false,
            "code_identities": [
                {
                    "code_identifier": "6ae3f4700fd07d529385148c34ed5c0b9a1c0727",
                    "code_identifier_type": "git",
                    "code_identifier_dependable": true,
                    "code_identifier_url": "INTENTIONALLY REMOVED",
                    "code_identifier_message": ""
                }
            ],
            "attempts": [
                {
                    "attempt_number": 0,
                    "start_time": "2023-02-01 12:12:26.533528",
                    "end_time": "2023-02-01 12:12:26.534091",
                    "duration": "0:00:00.000563",
                    "status": "SUCCESS",
                    "message": ""
                }
            ],
            "user_defined_metrics": {},
            "branches": {},
            "data_catalog": []
        },
        "step shell": {
            "name": "step shell",
            "internal_name": "step shell",
            "status": "SUCCESS",
            "step_type": "task",
            "message": "",
            "mock": false,
            "code_identities": [
                {
                    "code_identifier": "6ae3f4700fd07d529385148c34ed5c0b9a1c0727",
                    "code_identifier_type": "git",
                    "code_identifier_dependable": true,
                    "code_identifier_url": "INTENTIONALLY REMOVED",
                    "code_identifier_message": ""
                }
            ],
            "attempts": [
                {
                    "attempt_number": 0,
                    "start_time": "2023-02-01 12:12:29.287087",
                    "end_time": "2023-02-01 12:12:29.302014",
                    "duration": "0:00:00.014927",
                    "status": "SUCCESS",
                    "message": ""
                }
            ],
            "user_defined_metrics": {},
            "branches": {},
            "data_catalog": [
                {
                    "name": "data/data.txt",
                    "data_hash": "474c6f64a8bbbb97a7f01fb1207db9b27db04212ab437d4f495e2ac3f4be7388",
                    "catalog_relative_path": "demo-bash6/data/data.txt",
                    "catalog_handler_location": ".catalog",
                    "stage": "put"
                }
            ]
        },
        "success": {
            "name": "success",
            "internal_name": "success",
            "status": "SUCCESS",
            "step_type": "success",
            "message": "",
            "mock": false,
            "code_identities": [
                {
                    "code_identifier": "6ae3f4700fd07d529385148c34ed5c0b9a1c0727",
                    "code_identifier_type": "git",
                    "code_identifier_dependable": true,
                    "code_identifier_url": "INTENTIONALLY REMOVED",
                    "code_identifier_message": ""
                }
            ],
            "attempts": [
                {
                    "attempt_number": 0,
                    "start_time": "2023-02-01 12:12:32.083047",
                    "end_time": "2023-02-01 12:12:32.084351",
                    "duration": "0:00:00.001304",
                    "status": "SUCCESS",
                    "message": ""
                }
            ],
            "user_defined_metrics": {},
            "branches": {},
            "data_catalog": []
        }
    },
    "parameters": {
        "x": 4
    },
    "run_config": {
        "executor": {
            "type": "demo-renderer",
            "config": {
                "enable_parallel": false,
                "placeholders": {}
            }
        },
        "run_log_store": {
            "type": "file-system",
            "config": {
                "log_folder": ".run_log_store"
            }
        },
        "catalog": {
            "type": "file-system",
            "config": {
                "compute_data_folder": "data",
                "catalog_location": ".catalog"
            }
        },
        "secrets": {
            "type": "env-secrets-manager",
            "config": {}
        },
        "experiment_tracker": {
            "type": "mlflow",
            "config": {
                "server_url": "http://127.0.0.1:5000/",
                "autolog": true
            }
        },
        "variables": {},
        "pipeline": {
            "start_at": "step parameters",
            "name": "",
            "description": "Getting started",
            "max_time": 86400,
            "steps": {
                "step parameters": {
                    "mode_config": {},
                    "next_node": "step shell",
                    "command": "lambda x: {'x': int(x) + 1}",
                    "command_type": "python-lambda",
                    "command_config": {},
                    "catalog": {},
                    "retry": 1,
                    "on_failure": "",
                    "type": "task"
                },
                "step shell": {
                    "mode_config": {},
                    "next_node": "success",
                    "command": "mkdir data ; env >> data/data.txt",
                    "command_type": "shell",
                    "command_config": {},
                    "catalog": {
                        "put": [
                            "*"
                        ]
                    },
                    "retry": 1,
                    "on_failure": "",
                    "type": "task"
                },
                "success": {
                    "mode_config": {},
                    "type": "success"
                },
                "fail": {
                    "mode_config": {},
                    "type": "fail"
                }
            }
        }
    }
}
```

</details>

While the original run was in one single python process, the run via the bash uses a different python process for each
step of the dag. To extrapolate the idea, this is very similar to AWS step function execution or Argo dag execution that
every step of the pipeline executes either a AWS compute or a container.

Even though the process of execution of the nodes is different, the structure of run log/catalog is exactly identical
to ```local``` execution. This feature should enable you to debug/re-run a failed run in any other environments
in ```local``` environments.
