# Example Deployment

While the previous two sections were about introducing magnus pipelines and different features, we can use the same
trivial example to showcase the features of magnus in deployment patterns.

To recap, here is the pipeline that we ran as an example:

``` yaml
dag:
  description: Getting started
  start_at: step parameters
  steps:
    step parameters:
      type: task
      command_type: python-lambda
      command: "lambda x: {'x': int(x) + 1}"
      next: step shell
    step shell:
      type: task
      command_type: shell
      command: mkdir data ; env >> data/data.txt # For Linux/macOS
      next: success
      catalog:
        put:
          - "*"
    success:
      type: success
    fail:
      type: fail
```

The pipeline is simple and demonstrates the core concepts of data catalog, dag traversal, passing data between
nodes and task types.

To demonstrate the strength of magnus, let us try to "deploy" the pipeline via a Bash shell script. This demonstration,
though trivial, is very similar in process to translating a dag into something that argo or AWS step functions
understands.

Let us edit the ```getting-started.yaml``` file by adding these lines at the top:

```yaml
mode:
  type: demo-renderer

run_log_store:
  type: file-system

dag:
  description: Getting started
  start_at: step parameters
  steps:
    step parameters:
      type: task
      command_type: python-lambda
      command: "lambda x: {'x': int(x) + 1}"
      next: step shell
    step shell:
      type: task
      command_type: shell
      command: mkdir data ; env >> data/data.txt
      next: success
      catalog:
        put:
          - "*"
    success:
      type: success
    fail:
      type: fail

```

**Points to note**:

- We have not changed the dag definition at all.

- We added a config variable at the top which modifies the execution type to "demo-renderer".
[Demo renderer](../../concepts/modes-implementations/demo-renderer) translates the dag definition into a bash script.

- The buffered run log store that we have so far used in the example is not suitable anymore.
[File system](../../concepts/run-log-implementations/file-system/) run log store persists the logs
on physical folder and therefore more suitable.

There are other ways to change the configurations which are detailed [here](../../concepts/configurations).

## Translation

!!! warning "Changed in v0.2"

We can execute the pipeline, just like we did it previously, by the following command.

```magnus execute --file getting-started.yaml --parameters-file parameters.yaml```

This run is different from the previous execution

1. There is no output or run_id generated by magus. This is because the current execution only performs a translation
of the dag into a bash script and not actual function calls.

2. You should also notice a file called ```demo-bash.sh``` created in the working directory which is a
translation of the dag into a bash script.


Let us have a closer look at the contents of the ```demo-bash.sh```.

```shell
magnus execute_single_node $1 step%parameters --file getting-started.yaml
exit_code=$?
echo $exit_code
if [ $exit_code -ne 0 ];
then
	 $(magnus execute_single_node $1 fail --file getting-started.yaml)
	exit 1
fi
magnus execute_single_node $1 step%shell --file getting-started.yaml
exit_code=$?
echo $exit_code
if [ $exit_code -ne 0 ];
then
	 $(magnus execute_single_node $1 fail --file getting-started.yaml)
	exit 1
fi
magnus execute_single_node $1 success --file getting-started.yaml
```

The shell script does the following

- Capture the command line arguments passed to the bash script as magnus parameters, i.e prefixed by MAGNUS_PRM_.

- Execute the first node and capture the exit code. If the exit code is successful, move to ```next``` node as defined
in the dag.

- If the exit code is failure, move to the failure node of the dag. This is as per the dag definition.

## Execution

We can execute the pipeline defined in the ```demo-bash.sh``` by

```shell
chmod 755 demo-bash.sh

./demo-bash.sh my_first_bash
```

**Points to note**

1. run_id, my_first_bash, is no longer optional parameter and should be provided as the first positional parameter.

2). The parameters file was part of the translation step and is provided to the shell script.

Since the run log store is ```file-system```, there should be a directory, ```.run_log_store```, created with a single
run log in it by the name ```my_first_bash.json```.

<details>
  <summary>Click to show the run log</summary>

```json
{
    "run_id": "my_first_bash",
    "dag_hash": "ce0676d63e99c34848484f2df1744bab8d45e33a",
    "use_cached": false,
    "tag": null,
    "original_run_id": "",
    "status": "SUCCESS",
    "steps": {
        "step parameters": {
            "name": "step parameters",
            "internal_name": "step parameters",
            "status": "SUCCESS",
            "step_type": "task",
            "message": "",
            "mock": false,
            "code_identities": [
                {
                    "code_identifier": "493ae8c868fea18e50e6b6410f2c2290ab8d6734",
                    "code_identifier_type": "git",
                    "code_identifier_dependable": false,
                    "code_identifier_url": "<INTENTIONALLY REMOVED>",
                    "code_identifier_message": "<INTENTIONALLY REMOVED>"
                }
            ],
            "attempts": [
                {
                    "attempt_number": 0,
                    "start_time": "2022-01-19 08:23:46.720498",
                    "end_time": "2022-01-19 08:23:46.720987",
                    "duration": "0:00:00.000489",
                    "status": "SUCCESS",
                    "message": ""
                }
            ],
            "user_defined_metrics": {},
            "branches": {},
            "data_catalog": []
        },
        "step shell": {
            "name": "step shell",
            "internal_name": "step shell",
            "status": "SUCCESS",
            "step_type": "task",
            "message": "",
            "mock": false,
            "code_identities": [
                {
                    "code_identifier": "493ae8c868fea18e50e6b6410f2c2290ab8d6734",
                    "code_identifier_type": "git",
                    "code_identifier_dependable": false,
                    "code_identifier_url": "<INTENTIONALLY REMOVED>",
                    "code_identifier_message": "<INTENTIONALLY REMOVED>"
                }
            ],
            "attempts": [
                {
                    "attempt_number": 0,
                    "start_time": "2022-01-19 08:23:47.351849",
                    "end_time": "2022-01-19 08:23:47.377000",
                    "duration": "0:00:00.025151",
                    "status": "SUCCESS",
                    "message": ""
                }
            ],
            "user_defined_metrics": {},
            "branches": {},
            "data_catalog": [
                {
                    "name": "data.txt",
                    "data_hash": "011ba0c5de6693e544d838f7cd43f41ebe47b7a16053d17f3173f171c90579d6",
                    "catalog_relative_path": "my_first_bash/data.txt",
                    "catalog_handler_location": ".catalog",
                    "stage": "put"
                }
            ]
        },
        "success": {
            "name": "success",
            "internal_name": "success",
            "status": "SUCCESS",
            "step_type": "success",
            "message": "",
            "mock": false,
            "code_identities": [
                {
                    "code_identifier": "493ae8c868fea18e50e6b6410f2c2290ab8d6734",
                    "code_identifier_type": "git",
                    "code_identifier_dependable": false,
                    "code_identifier_url": "<INTENTIONALLY REMOVED>",
                    "code_identifier_message": "<INTENTIONALLY REMOVED>"
                }
            ],
            "attempts": [
                {
                    "attempt_number": 0,
                    "start_time": "2022-01-19 08:23:48.015055",
                    "end_time": "2022-01-19 08:23:48.016062",
                    "duration": "0:00:00.001007",
                    "status": "SUCCESS",
                    "message": ""
                }
            ],
            "user_defined_metrics": {},
            "branches": {},
            "data_catalog": []
        }
    },
    "parameters": {
        "x": 4
    },
    "run_config": {
        "executor": {
            "type": "demo-renderer",
            "config": {}
        },
        "run_log_store": {
            "type": "file-system",
            "config": {}
        },
        "catalog": {
            "type": "file-system",
            "config": {}
        },
        "secrets": {
            "type": "do-nothing",
            "config": {}
        }
    }
}
```

</details>

While the original run was in one single python process, the run via the bash uses a different python process for each
step of the dag. To extrapolate the idea, this is very similar to AWS step function execution or Argo dag execution that
every step of the pipeline executes either a AWS compute or a container.

Even though the process of execution of the nodes is different, the structure of run log/catalog is exactly identical
to ```local``` execution. This feature should enable you to debug/re-run a failed run in any other environments
in ```local``` environments.
